| Sr. No. | Topic                                             | What it is?                                                                                                   | What it is used for?                                                                                            | How it is used?                                                                                                   | Real-Life Examples                                                                                      |
|---------|---------------------------------------------------|---------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| 1       | **PCA (Principal Component Analysis)**           | A dimensionality reduction technique that transforms data into a new basis aligned with directions of maximum variance. | Used to reduce the dimensionality of datasets while preserving as much variability as possible.                 | Compute the covariance matrix, find its eigenvectors/eigenvalues, and transform data to the new basis. | Image compression, feature reduction in machine learning, and noise removal in signal processing.      |
| 2       | **Dimensionality Reduction**                     | The process of reducing the number of features or variables in a dataset while retaining its key characteristics.       | Simplifies models, reduces computation, and avoids overfitting.                                                  | Methods include PCA, t-SNE, and autoencoders.                                                          | Used in high-dimensional datasets like images, text, and genomics.                                    |
| 3       | **Mean of Dataset**                              | The average value of a dataset, calculated as the sum of all values divided by the total number of values.             | Represents the central tendency of the data.                                                                     | Calculated as `mean = Σx / n`.                                                                         | Used in summarizing data in statistics, economics, and science.                                       |
| 4       | **Variance of Dataset**                          | A measure of the spread or dispersion of data values from the mean.                                                   | Indicates how much the data points differ from the average value.                                                | Calculated as `variance = Σ(x - mean)^2 / n`.                                                           | Analyzing variability in stock prices, experiments, or manufacturing processes.                       |
| 5       | **Standard Deviation of Dataset**                | The square root of variance, providing a measure of spread in the same units as the data.                              | Used to quantify the amount of variation or dispersion in a dataset.                                             | Calculated as `std = √variance`.                                                                      | Assessing risk in finance or consistency in product quality.                                          |
| 6       | **Symmetric, Positive Definite (Covariance Matrix)** | A matrix representing pairwise covariances of variables, always symmetric and positive definite for valid datasets.     | Ensures stability in computations like PCA and multivariate statistics.                                           | Formed as `Σ = E[(X - μ)(X - μ)^T]`.                                                                   | Used in machine learning, multivariate analysis, and signal processing.                              |
| 7       | **Covariance of Dataset**                        | A measure of the linear relationship between two variables.                                                             | Helps determine how two variables change together.                                                               | Calculated as `cov(X, Y) = Σ[(X - μ_X)(Y - μ_Y)] / n`.                                                  | Used in portfolio optimization and understanding relationships between variables.                      |
| 8       | **Covariance Matrix**                            | A matrix that encapsulates covariances between all pairs of variables in a dataset.                                      | Used in multivariate analysis, PCA, and understanding relationships in data.                                     | Computed from datasets as `Σ = E[(X - μ)(X - μ)^T]`.                                                   | Applications in data science, physics, and natural language processing.                              |
| 9       | **Correlation**                                  | A normalized measure of the strength and direction of a linear relationship between two variables.                      | Helps quantify relationships between variables on a [-1, 1] scale.                                               | Calculated as `corr(X, Y) = cov(X, Y) / (σ_X * σ_Y)`.                                                  | Used in feature selection, finance, and epidemiology.                                                 |
| 10      | **Covariance vs. Correlation**                   | Covariance measures linear relationships (in units of data), while correlation is a normalized and unitless version.    | Correlation is used when scale-invariant relationships are needed.                                               | Correlation normalizes covariance using standard deviations.                                            | Correlation is preferred for comparisons across datasets, while covariance is useful in PCA.         |
| 11      | **Effect of Linear or Affine Transformation**     | Linear transformations (scaling, rotation) or affine transformations (translation) affect mean, variance, and covariance. | Affects statistical properties of datasets, altering interpretation and analysis.                                 | Mean shifts by translation; variance scales by factor squared; covariance transforms by the scaling factor. | Used in feature scaling, image processing, and signal transformations.                                |
| 12 | Dot Product                          | A mathematical operation that takes two equal-length sequences of numbers and returns a single number. In data science, it is used to compute projections or determine orthogonality.                    | Used to calculate angles, projections, or test orthogonality between vectors in ML.                                                     | Compute as \( \mathbf{a} \cdot \mathbf{b} = \sum_{i} a_i b_i \). | Example: Finding the cosine similarity between two vectors in a recommendation system.                            |
| 13 | Angle, Length, and Distance using Dot Product | Measures angle, magnitude (length), and the geometric relationship between two vectors.                                                        | Used in geometric analysis, feature normalization, or finding similarity in high-dimensional data.                      | \( \cos \theta = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|} \). Length: \( \|\mathbf{a}\| = \sqrt{\mathbf{a} \cdot \mathbf{a}} \). Distance: \( \|\mathbf{a} - \mathbf{b}\| \). | Example: Computing the similarity between document embeddings.                                                      |
| 14 | Inner Product                        | A generalization of the dot product to higher-dimensional or abstract spaces, used for functions or matrices.                                         | Used in kernels, projections, and distance calculations in high-dimensional spaces in ML.                                 | Defined as \( \langle \mathbf{a}, \mathbf{b} \rangle = \mathbf{a}^T A \mathbf{b} \), where \( A \) is a positive-definite matrix. | Example: Calculating angles in a non-Euclidean space or applying kernels in SVMs.                                   |
| 15 | Inner vs Dot Product                 | Dot product is specific to Euclidean space, while the inner product generalizes it for non-Euclidean spaces or other vector fields.                  | Differentiates between operations in simple vector spaces vs abstract spaces (e.g., Hilbert spaces).                      | The inner product generalizes: \( \langle \mathbf{a}, \mathbf{b} \rangle = \mathbf{a}^T A \mathbf{b} \).              | Example: Using the inner product in signal processing to calculate orthogonality of signals.                         |
| 16 | Bilinear                             | A property of a function where linearity holds in each argument separately, such as in inner products.                                                 | Used to ensure operations like matrix-vector multiplication or inner products are valid in ML/DS models.                 | \( \langle a\mathbf{x}, \mathbf{y} \rangle = a \langle \mathbf{x}, \mathbf{y} \rangle \) and \( \langle \mathbf{x}, \mathbf{y}_1 + \mathbf{y}_2 \rangle = \langle \mathbf{x}, \mathbf{y}_1 \rangle + \langle \mathbf{x}, \mathbf{y}_2 \rangle \). | Example: Ensures kernel functions or similarity functions satisfy linearity.                                         |
| 17 | Symmetric                            | A property where \( \langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle \), ensuring interchangeability of arguments.         | Used in covariance, kernel matrices, or symmetric tensors in ML.                                                         | Defined as \( A = A^T \) for matrices or tensors.                                                                      | Example: Ensures the covariance matrix or kernel functions in SVMs are symmetric.                                    |
| 18 | Positive Definite                    | A property where \( \langle \mathbf{x}, \mathbf{x} \rangle > 0 \) for all \( \mathbf{x} \neq 0 \), ensuring meaningful inner product spaces.               | Used to ensure stability and meaningful results in optimization or metric learning.                                       | \( \mathbf{x}^T A \mathbf{x} > 0 \) for all \( \mathbf{x} \neq 0 \).                                                    | Example: Guaranteeing Hessians or covariance matrices are positive definite for gradient-based methods.               |
| 19 | Angle, Length, and Distance using Inner Product | Generalization of angle, length, and distance calculations to inner product spaces.                                                                   | Used for geometric interpretation in high-dimensional data or non-Euclidean spaces in ML.                                | Angle: \( \cos \theta = \frac{\langle \mathbf{a}, \mathbf{b} \rangle}{\sqrt{\langle \mathbf{a}, \mathbf{a} \rangle \langle \mathbf{b}, \mathbf{b} \rangle}} \). | Example: Calculating similarities in reproducing kernel Hilbert spaces for support vector machines (SVMs).           |
