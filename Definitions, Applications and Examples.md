| Sr. No. | Topic                                             | What it is?                                                                                                   | What it is used for?                                                                                            | How it is used?                                                                                                   | Real-Life Examples                                                                                      |
|---------|---------------------------------------------------|---------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| 1       | **PCA (Principal Component Analysis)**           | A dimensionality reduction technique that transforms data into a new basis aligned with directions of maximum variance. | Used to reduce the dimensionality of datasets while preserving as much variability as possible.                 | Compute the covariance matrix, find its eigenvectors/eigenvalues, and transform data to the new basis. | Image compression, feature reduction in machine learning, and noise removal in signal processing.      |
| 2       | **Dimensionality Reduction**                     | The process of reducing the number of features or variables in a dataset while retaining its key characteristics.       | Simplifies models, reduces computation, and avoids overfitting.                                                  | Methods include PCA, t-SNE, and autoencoders.                                                          | Used in high-dimensional datasets like images, text, and genomics.                                    |
| 3       | **Mean of Dataset**                              | The average value of a dataset, calculated as the sum of all values divided by the total number of values.             | Represents the central tendency of the data.                                                                     | Calculated as `mean = Σx / n`.                                                                         | Used in summarizing data in statistics, economics, and science.                                       |
| 4       | **Variance of Dataset**                          | A measure of the spread or dispersion of data values from the mean.                                                   | Indicates how much the data points differ from the average value.                                                | Calculated as `variance = Σ(x - mean)^2 / n`.                                                           | Analyzing variability in stock prices, experiments, or manufacturing processes.                       |
| 5       | **Standard Deviation of Dataset**                | The square root of variance, providing a measure of spread in the same units as the data.                              | Used to quantify the amount of variation or dispersion in a dataset.                                             | Calculated as `std = √variance`.                                                                      | Assessing risk in finance or consistency in product quality.                                          |
| 6       | **Symmetric, Positive Definite (Covariance Matrix)** | A matrix representing pairwise covariances of variables, always symmetric and positive definite for valid datasets.     | Ensures stability in computations like PCA and multivariate statistics.                                           | Formed as `Σ = E[(X - μ)(X - μ)^T]`.                                                                   | Used in machine learning, multivariate analysis, and signal processing.                              |
| 7       | **Covariance of Dataset**                        | A measure of the linear relationship between two variables.                                                             | Helps determine how two variables change together.                                                               | Calculated as `cov(X, Y) = Σ[(X - μ_X)(Y - μ_Y)] / n`.                                                  | Used in portfolio optimization and understanding relationships between variables.                      |
| 8       | **Covariance Matrix**                            | A matrix that encapsulates covariances between all pairs of variables in a dataset.                                      | Used in multivariate analysis, PCA, and understanding relationships in data.                                     | Computed from datasets as `Σ = E[(X - μ)(X - μ)^T]`.                                                   | Applications in data science, physics, and natural language processing.                              |
| 9       | **Correlation**                                  | A normalized measure of the strength and direction of a linear relationship between two variables.                      | Helps quantify relationships between variables on a [-1, 1] scale.                                               | Calculated as `corr(X, Y) = cov(X, Y) / (σ_X * σ_Y)`.                                                  | Used in feature selection, finance, and epidemiology.                                                 |
| 10      | **Covariance vs. Correlation**                   | Covariance measures linear relationships (in units of data), while correlation is a normalized and unitless version.    | Correlation is used when scale-invariant relationships are needed.                                               | Correlation normalizes covariance using standard deviations.                                            | Correlation is preferred for comparisons across datasets, while covariance is useful in PCA.         |
| 11      | **Effect of Linear or Affine Transformation**     | Linear transformations (scaling, rotation) or affine transformations (translation) affect mean, variance, and covariance. | Affects statistical properties of datasets, altering interpretation and analysis.                                 | Mean shifts by translation; variance scales by factor squared; covariance transforms by the scaling factor. | Used in feature scaling, image processing, and signal transformations.                                |
| 12 | **Dot Product** | The dot product of two vectors is a scalar that measures their alignment. | It is used to compute angles, lengths, and projections in geometric and data science applications. | Multiply corresponding components of two vectors and sum the results. | Formula: `a · b = Σ (aᵢ * bᵢ)` |
| 13 | **Angle, Length and Distance Using Dot Product** | The dot product helps calculate angles, vector magnitudes, and distances. | Used in machine learning to calculate similarity and orthogonality of vectors. | Compute the cosine of the angle using the dot product and vector norms. | Formula: `cos(θ) = (a · b) / (\|a\| * \|b\|)` |
| 14 | **Inner Product** | A generalization of the dot product defined for any vector space. | Useful in defining angles, lengths, and projections in high-dimensional spaces. | Defined using a symmetric, bilinear, and positive-definite matrix. | Formula: `⟨x, y⟩ = xᵀ A y` |
| 15 | **Inner vs Dot Product** | The inner product generalizes the dot product using a matrix. | Helps extend geometric concepts like angles and orthogonality to advanced spaces. | Inner product is defined via a positive-definite matrix; the dot product is a special case. | No specific formula; generalization depends on the matrix used in the inner product. |
| 16 | **Bilinear** | A function linear in both arguments, often part of inner product definitions. | Ensures operations like scaling and addition are consistent. | Multiplies elements of two vectors with properties of linearity. | Example: `⟨x, y⟩ = xᵀ A y` satisfies bilinearity. |
| 17 | **Symmetric** | A property where the result remains the same when arguments are swapped. | Ensures the inner product adheres to geometric intuition. | Inner product satisfies `⟨x, y⟩ = ⟨y, x⟩`. | Example: `⟨x, y⟩ = xᵀ A y` with symmetric matrix A. |
| 18 | **Positive Definite** | Ensures the inner product is always non-negative. | Critical in defining meaningful lengths and distances. | Inner product is positive unless vectors are zero. | Example: `xᵀ A x > 0` for all non-zero x with positive-definite matrix A. |
| 19 | **Angle, Length and Distance Using Inner Product** | Use the inner product to define angles, lengths, and distances in advanced spaces. | Extends dot product concepts to high-dimensional vector spaces. | Compute `cos(θ)`, lengths, or projections using the inner product. | Formula: `cos(θ) = ⟨x, y⟩ / (√⟨x, x⟩ * √⟨y, y⟩)` |
